# ================================
# All-in-one: Ollama + App
# ================================

FROM python:3.10-slim

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    UV_LINK_MODE=copy \
    CONFIG_FILE=/app/conf/conf.yaml \
    OLLAMA_HOST=127.0.0.1:11434

WORKDIR /app

# ------------------------------------------------
# System deps
# ------------------------------------------------
RUN apt-get update -o Acquire::Retries=5 \
 && apt-get install -y --no-install-recommends \
      ffmpeg git curl ca-certificates \
 && rm -rf /var/lib/apt/lists/*

# ------------------------------------------------
# uv
# ------------------------------------------------
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /usr/local/bin/

# ------------------------------------------------
# Ollama binary
# ------------------------------------------------
COPY --from=ollama/ollama:latest /bin/ollama /usr/local/bin/ollama

# ------------------------------------------------
# Python deps (cached)
# ------------------------------------------------
COPY pyproject.toml uv.lock ./
RUN --mount=type=cache,target=/root/.cache/uv \
    uv sync --frozen --no-dev

# ------------------------------------------------
# App code
# ------------------------------------------------
COPY . /app
RUN uv pip install --no-deps .

# ------------------------------------------------
# Entrypoint
# ------------------------------------------------
RUN cat > /usr/local/bin/start-all <<'EOF'
#!/usr/bin/env sh
set -eu

echo "[BOOT] Starting container"

# ------------------------------------------------
# 0) Config
# ------------------------------------------------
if [ ! -f "/app/conf/conf.yaml" ]; then
  echo "[ERROR] /app/conf/conf.yaml is required" >&2
  exit 1
fi

ln -sf /app/conf/conf.yaml /app/conf.yaml

# Optional model_dict.json
if [ -f "/app/conf/model_dict.json" ]; then
  ln -sf /app/conf/model_dict.json /app/model_dict.json
elif [ -f "/app/config_templates/model_dict.json" ]; then
  cp /app/config_templates/model_dict.json /app/model_dict.json
fi

# Optional resource dirs
for d in live2d-models characters avatars backgrounds; do
  if [ -d "/app/conf/$d" ]; then
    rm -rf "/app/$d"
    ln -s "/app/conf/$d" "/app/$d"
  elif [ -d "/app/config_templates/$d" ]; then
    cp -r "/app/config_templates/$d" "/app/$d"
  fi
done

# ------------------------------------------------
# 1) Start Ollama
# ------------------------------------------------
echo "[BOOT] Starting Ollama..."
ollama serve >/var/log/ollama.log 2>&1 &

# Wait until ready
for i in $(seq 1 60); do
  if curl -fsS http://127.0.0.1:11434/v1/models >/dev/null 2>&1; then
    echo "[BOOT] Ollama ready"
    break
  fi
  sleep 1
  if [ "$i" -eq 60 ]; then
    echo "[ERROR] Ollama failed to start" >&2
    exit 1
  fi
done

# ------------------------------------------------
# 2) Read YAML config
# ------------------------------------------------
eval "$(
python - <<'PY'
import yaml

with open("/app/conf/conf.yaml", "r") as f:
    cfg = yaml.safe_load(f) or {}

agent_cfg = cfg.get("character_config", {}).get("agent_config", {})
provider = agent_cfg.get("agent_settings", {}) \
                    .get("basic_memory_agent", {}) \
                    .get("llm_provider", "")

model = agent_cfg.get("llm_configs", {}) \
                 .get(provider, {}) \
                 .get("model", "")

print(f'LLM_PROVIDER="{provider}"')
print(f'LLM_MODEL="{model}"')
PY
)"

echo "[CONF] LLM_PROVIDER=$LLM_PROVIDER"
echo "[CONF] LLM_MODEL=$LLM_MODEL"

# ------------------------------------------------
# 3) Ollama bootstrap
# ------------------------------------------------
if [ "$LLM_PROVIDER" = "ollama_llm" ]; then
  if [ -z "$LLM_MODEL" ]; then
    echo "[ERROR] ollama_llm requires model" >&2
    exit 1
  fi

  echo "[OLLAMA] Keep model: $LLM_MODEL"

  # Remove other models only
  ollama list 2>/dev/null | awk '{print $1}' | grep -v '^Name$' | while read -r m; do
    if [ "$m" != "$LLM_MODEL" ]; then
      echo "[OLLAMA] Removing unused model: $m"
      ollama rm "$m" || true
    fi
  done

  if ! ollama list 2>/dev/null | awk '{print $1}' | grep -qx "$LLM_MODEL"; then
    echo "[OLLAMA] Pulling model: $LLM_MODEL"
    ollama pull "$LLM_MODEL"
  else
    echo "[OLLAMA] Model already cached"
  fi

  echo "[OLLAMA] Warming up model"
  ollama run "$LLM_MODEL" </dev/null || true
else
  echo "[OLLAMA] Provider is not ollama, skipping bootstrap"
fi

# ------------------------------------------------
# 4) Start app
# ------------------------------------------------
echo "[BOOT] Starting application"
exec uv run run_server.py
EOF

# Fix line endings and permissions
RUN sed -i 's/\r$//' /usr/local/bin/start-all \
    && chmod +x /usr/local/bin/start-all

# ------------------------------------------------
# Volumes
# ------------------------------------------------
VOLUME ["/root/.ollama", "/app/conf"]

# App port
EXPOSE 12393

CMD ["/usr/local/bin/start-all"]
