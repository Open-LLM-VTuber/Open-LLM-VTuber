# ================================
# All-in-one: App (+ optional Ollama)
# ================================

FROM python:3.10-slim

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    UV_LINK_MODE=copy \
    CONFIG_FILE=/app/conf/conf.yaml

WORKDIR /app

# ------------------------------------------------
# System deps
# ------------------------------------------------
RUN apt-get update -o Acquire::Retries=5 \
 && apt-get install -y --no-install-recommends \
      ffmpeg git curl ca-certificates \
 && rm -rf /var/lib/apt/lists/*

# ------------------------------------------------
# uv binary
# ------------------------------------------------
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /usr/local/bin/

# ------------------------------------------------
# Ollama binary (only binary, no daemon yet)
# ------------------------------------------------
COPY --from=ollama/ollama:latest /bin/ollama /usr/local/bin/ollama

# ------------------------------------------------
# Python deps (cached)
# ------------------------------------------------
COPY pyproject.toml uv.lock ./
RUN --mount=type=cache,target=/root/.cache/uv \
    uv sync --frozen --no-dev

# ------------------------------------------------
# App code
# ------------------------------------------------
COPY . /app
RUN uv pip install --no-deps .

# ------------------------------------------------
# Entrypoint
# ------------------------------------------------
RUN cat > /usr/local/bin/start-all <<'EOF'
#!/usr/bin/env sh
set -eu

echo "[BOOT] Starting container"

# ------------------------------------------------
# 0) Config check
# ------------------------------------------------
if [ ! -f "/app/conf/conf.yaml" ]; then
  echo "[ERROR] /app/conf/conf.yaml is required" >&2
  exit 1
fi

ln -sf /app/conf/conf.yaml /app/conf.yaml

# ------------------------------------------------
# 1) Resource directories (ONLY from conf)
# ------------------------------------------------
for d in live2d-models characters avatars backgrounds; do
  if [ -d "/app/conf/$d" ]; then
    rm -rf "/app/$d"
    ln -s "/app/conf/$d" "/app/$d"
    echo "[CONF] Linked $d"
  fi
done

# ------------------------------------------------
# 2) Read YAML config
# ------------------------------------------------
LLM_PROVIDER=""
LLM_MODEL=""

eval "$(
uv run - <<'PY'
import yaml

with open("/app/conf/conf.yaml", "r") as f:
    cfg = yaml.safe_load(f) or {}

agent_cfg = cfg.get("character_config", {}).get("agent_config", {})
provider = agent_cfg.get("agent_settings", {}) \
                    .get("basic_memory_agent", {}) \
                    .get("llm_provider", "")

model = agent_cfg.get("llm_configs", {}) \
                 .get(provider, {}) \
                 .get("model", "")

print(f'LLM_PROVIDER="{provider}"')
print(f'LLM_MODEL="{model}"')
PY
)"

echo "[CONF] LLM_PROVIDER=$LLM_PROVIDER"
echo "[CONF] LLM_MODEL=$LLM_MODEL"

# ------------------------------------------------
# 3) Start Ollama ONLY if needed
# ------------------------------------------------
if [ "$LLM_PROVIDER" = "ollama_llm" ]; then
  if [ -z "$LLM_MODEL" ]; then
    echo "[ERROR] LLM_PROVIDER=ollama_llm but model is empty" >&2
    exit 1
  fi

  echo "[BOOT] Starting Ollama (serve only)..."
  ollama serve >/var/log/ollama.log 2>&1 &

  # Wait for Ollama API
  for i in $(seq 1 60); do
    if curl -fsS http://127.0.0.1:11434/api/tags >/dev/null 2>&1; then
      echo "[BOOT] Ollama ready"
      break
    fi
    sleep 1
    if [ "$i" -eq 60 ]; then
      echo "[ERROR] Ollama failed to start" >&2
      exit 1
    fi
  done

  # ----------------------------------------------
  # Pull model (REQUIRED even if we don't run it)
  # ----------------------------------------------
  echo "[BOOT] Ensuring Ollama model exists: $LLM_MODEL"
  ollama pull "$LLM_MODEL"

else
  echo "[BOOT] LLM provider is $LLM_PROVIDER, skipping Ollama"
fi

# ------------------------------------------------
# 4) Start app
# ------------------------------------------------
echo "[BOOT] Starting application"
exec uv run run_server.py
EOF

# Fix CRLF + permissions
RUN sed -i 's/\r$//' /usr/local/bin/start-all \
 && chmod +x /usr/local/bin/start-all

# ------------------------------------------------
# Volumes
# ------------------------------------------------
VOLUME ["/root/.ollama", "/app/conf"]

EXPOSE 12393

CMD ["/usr/local/bin/start-all"]
